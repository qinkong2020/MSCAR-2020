{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88Uyincoehmy"
   },
   "source": [
    "# Introduction to Machine Learning using `scikit-learn`\n",
    "\n",
    "Here is an example of using various regressional techniques (e.g., linear regression, random forest) to predict the maximum temperature from previous weather data.\n",
    "\n",
    "Note: This is closely following the example provide from this [source](https://towardsdatascience.com/random-forest-in-python-24d0893d51c0)\n",
    " \n",
    "---\n",
    "\n",
    "### Goal:\n",
    "The goal of this notebook is to provide a working example of how to predict a desired quantity using a open-source packaged called [scikit-learn](https://scikit-learn.org/stable/)\n",
    "\n",
    "### Exact Task: \n",
    "\n",
    "Much like what everyone does in the WxChallenge, we are tasked to predict the next days maximum temperature from the previous days temperature and other features. Maybe you can be inspired to translate this into a sucessful effort in the WxChallenge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8FbgnfHfboT"
   },
   "source": [
    "### Step 1: \n",
    "\n",
    "Let us load in some pre-downloaded data from Seattle, WA. We will use pandas, a helpful package that can go grab the data hosted on github for use and return a DataFrame (which we will name df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 405,
     "status": "ok",
     "timestamp": 1601117368192,
     "user": {
      "displayName": "Stephen Nesbitt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQLj1729l6nTV5bTwtz3dzPigxe5VgnxpGo5QCiuk=s64",
      "userId": "06234418338196972854"
     },
     "user_tz": 300
    },
    "id": "RP86NL6neN6K"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/dopplerchase/WeatherScripts/master/temps.csv',parse_dates=[['year','month','day']])\n",
    "df = df.drop(labels='week',axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbv7rOFZh30l"
   },
   "source": [
    "Let's take a gander at what is in this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "executionInfo": {
     "elapsed": 588,
     "status": "ok",
     "timestamp": 1601117370456,
     "user": {
      "displayName": "Stephen Nesbitt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQLj1729l6nTV5bTwtz3dzPigxe5VgnxpGo5QCiuk=s64",
      "userId": "06234418338196972854"
     },
     "user_tz": 300
    },
    "id": "LSFKVSMihcc7",
    "outputId": "f19d6cff-4cd0-486d-e70f-dc4f74a8cdad"
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfnoSMJ0h-gy"
   },
   "source": [
    "Here is the description of the variable names:\n",
    "\n",
    "[0] year: 2016 for all data points <br>\n",
    "[1] month: number for month of the year <br>\n",
    "[2] day: number for day of the year <br>\n",
    "[3] week: day of the week as a character string <br>\n",
    "[4] temp_2: max temperature 2 days prior <br>\n",
    "[5] temp_1: max temperature 1 day prior <br>\n",
    "[6] average: historical average max temperature <br>\n",
    "[7] actual: max temperature measurement <br>\n",
    "[8] friend: your friendâ€™s prediction, a random number between 20 below the average and 20 above the average <br>\n",
    "\n",
    "### Step 2:\n",
    "\n",
    "Lets plot one of the columns just to see what the data looks like. Specifically, the column we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "executionInfo": {
     "elapsed": 1069,
     "status": "ok",
     "timestamp": 1601115967059,
     "user": {
      "displayName": "Stephen Nesbitt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQLj1729l6nTV5bTwtz3dzPigxe5VgnxpGo5QCiuk=s64",
      "userId": "06234418338196972854"
     },
     "user_tz": 300
    },
    "id": "7_H9iIbth82o",
    "outputId": "de4e462d-a654-4f58-fbd1-821ab4dc161c"
   },
   "outputs": [],
   "source": [
    "#pylab inline imports numpy as np and matplotlib.pyplot as plt in one easy step. \n",
    "%pylab inline\n",
    "\n",
    "#this line makes the plots nice and high resolution!\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "#plot parameters that I personally like, feel free to make these your own.\n",
    "matplotlib.rcParams['axes.facecolor'] = [0.9,0.9,0.9]\n",
    "matplotlib.rcParams['axes.labelsize'] = 14\n",
    "matplotlib.rcParams['axes.titlesize'] = 14\n",
    "matplotlib.rcParams['xtick.labelsize'] = 12\n",
    "matplotlib.rcParams['ytick.labelsize'] = 12\n",
    "matplotlib.rcParams['legend.fontsize'] = 12\n",
    "matplotlib.rcParams['legend.facecolor'] = 'w'\n",
    "matplotlib.rcParams['figure.facecolor'] ='w'\n",
    "\n",
    "#Im gonna make a datetime to make ploting nice and easy \n",
    "import datetime \n",
    "dtime = np.zeros(df.day.shape[0],dtype=object)\n",
    "for i in np.arange(df.day.shape[0]):\n",
    "  dtime[i] = datetime.datetime(df.year.values[i],df.month.values[i],df.day.values[i])\n",
    "\n",
    "dtime_index = pd.to_datetime(dtime)\n",
    "\n",
    "#lets shove it into the dataframe\n",
    "df['dtime_index'] = dtime_index\n",
    "\n",
    "#HERE IS THE ACTUAL PLOTING!\n",
    "plt.plot(df.dtime_index,df.actual)\n",
    "_ = plt.xticks(rotation=45)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcarOwqJnzwe"
   },
   "source": [
    "###Step 3: Your Turn  \n",
    "\n",
    "I think it would be wise to take a gander at a couple of the columns that you will use as predictors, in data science, these are called Features. \n",
    "\n",
    "\n",
    "For example please plot the temp_1 and temp_2 variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1252,
     "status": "ok",
     "timestamp": 1601115967258,
     "user": {
      "displayName": "Stephen Nesbitt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQLj1729l6nTV5bTwtz3dzPigxe5VgnxpGo5QCiuk=s64",
      "userId": "06234418338196972854"
     },
     "user_tz": 300
    },
    "id": "LIfkMSESi-Lw"
   },
   "outputs": [],
   "source": [
    "#put your code here, feel free to copy from above\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1X0dS23oMDS"
   },
   "source": [
    "You should see one glaring issue with this data! It should never be 117 $^\\circ$F in Seattle, let alone in November. This is a valuable lesson! In order to get good predictions you need to give your model good data. \n",
    "\n",
    "I would drop this row from your data. You can do this by the following method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1245,
     "status": "ok",
     "timestamp": 1601115967260,
     "user": {
      "displayName": "Stephen Nesbitt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQLj1729l6nTV5bTwtz3dzPigxe5VgnxpGo5QCiuk=s64",
      "userId": "06234418338196972854"
     },
     "user_tz": 300
    },
    "id": "_lqDrIlBoItb"
   },
   "outputs": [],
   "source": [
    "df = df.where(df.temp_1 < 100).dropna(how='all')\n",
    "df = df.where(df.temp_2 < 100).dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9sG9Rg4pUuj"
   },
   "source": [
    "### Step 4: \n",
    "\n",
    "Now that we cleaned up the bad data point, we need to massage the data into a format the scikit uses. This format is the same for all its models and that is\n",
    "\n",
    "X: matrix, (n_samples,n_features) [this is what we use to predict] <br>\n",
    "y: array, (n_samples) [this is what you want to predict] <br>\n",
    "\n",
    "To keep it simple to start, let us just use the previous day's temp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1234,
     "status": "ok",
     "timestamp": 1601115967261,
     "user": {
      "displayName": "Stephen Nesbitt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQLj1729l6nTV5bTwtz3dzPigxe5VgnxpGo5QCiuk=s64",
      "userId": "06234418338196972854"
     },
     "user_tz": 300
    },
    "id": "IICtB8mZpTUl",
    "outputId": "bdfcb79f-21b4-4cd5-e9fa-8b3ccea5fd82"
   },
   "outputs": [],
   "source": [
    "#you might think this is the way to go first\n",
    "X = df.temp_1\n",
    "y = df.actual\n",
    "#but look at the shapes.. \n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVnCNiA1rJg3"
   },
   "source": [
    "the y variable is fine, but you need to add the other dimension to X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1223,
     "status": "ok",
     "timestamp": 1601115967264,
     "user": {
      "displayName": "Stephen Nesbitt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQLj1729l6nTV5bTwtz3dzPigxe5VgnxpGo5QCiuk=s64",
      "userId": "06234418338196972854"
     },
     "user_tz": 300
    },
    "id": "AtuVYP0BoSpZ",
    "outputId": "eb2ce85e-6664-437e-f282-b3259e80234f"
   },
   "outputs": [],
   "source": [
    "X = df.temp_1[:,np.newaxis]\n",
    "y = df.actual\n",
    "#thats better \n",
    "X.shape,y.shape\n",
    "\n",
    "#now we are ready to grab scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZXUTWfXrffY"
   },
   "source": [
    "### Step 5:\n",
    "\n",
    "For any type of machine learning model you need to have a training dataset and testing dataset. This is how we will tell if our model is overfit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1565,
     "status": "ok",
     "timestamp": 1601115967620,
     "user": {
      "displayName": "Stephen Nesbitt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQLj1729l6nTV5bTwtz3dzPigxe5VgnxpGo5QCiuk=s64",
      "userId": "06234418338196972854"
     },
     "user_tz": 300
    },
    "id": "kRRSG85os1RR",
    "outputId": "f3f70b59-fe6e-4396-adb6-8376e0ce37b9"
   },
   "outputs": [],
   "source": [
    "# to do the splitting, we can use the  train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
    "\n",
    "#see the new sizes\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1J1BrcZuelK"
   },
   "source": [
    "### Step 6: \n",
    "\n",
    "Use climatology to get some baseline statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1549,
     "status": "ok",
     "timestamp": 1601115967622,
     "user": {
      "displayName": "Stephen Nesbitt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQLj1729l6nTV5bTwtz3dzPigxe5VgnxpGo5QCiuk=s64",
      "userId": "06234418338196972854"
     },
     "user_tz": 300
    },
    "id": "xjpSQna7s8dD",
    "outputId": "082eda1e-da6c-4257-f770-07be1ab6a67c"
   },
   "outputs": [],
   "source": [
    "mean_abs_error = np.mean(np.abs(df.actual - df.average))\n",
    "print('Using Climo gives us a mean error of: {}'.format(mean_abs_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uemu2_2Uv_Nh"
   },
   "source": [
    "Looks like for a regression to have any skill it has to do better than 4.94 degrees!\n",
    "\n",
    "### Step 7: \n",
    "\n",
    "Lets do a linear regression first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "executionInfo": {
     "elapsed": 1943,
     "status": "ok",
     "timestamp": 1601115968030,
     "user": {
      "displayName": "Stephen Nesbitt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQLj1729l6nTV5bTwtz3dzPigxe5VgnxpGo5QCiuk=s64",
      "userId": "06234418338196972854"
     },
     "user_tz": 300
    },
    "id": "o07kESmhuocC",
    "outputId": "0cdbd176-561b-47a8-8e0b-17f35175f474"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#to train this is all we have to do! reg stands for regression\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "#to predict do the following \n",
    "yhat = reg.predict(X_test)\n",
    "\n",
    "#lets plot the results \n",
    "plt.plot(yhat,y_test,'.')\n",
    "\n",
    "mean_abs_error = np.mean(np.abs(y_test - yhat))\n",
    "print('Using Linear Reg. gives us a mean error of: {}'.format(mean_abs_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgwghlZmxTVy"
   },
   "source": [
    "Slightly better results using the previous days temperature! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_AxdcN0Xx1t2"
   },
   "source": [
    "### Step 8: \n",
    "\n",
    "Let us try something a bit more sophistocated, like a random forest! if you are interested in learning about the method please consult this [source](https://towardsdatascience.com/understanding-random-forest-58381e0602d2#:~:text=The%20random%20forest%20is%20a,that%20of%20any%20individual%20tree.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "executionInfo": {
     "elapsed": 2538,
     "status": "ok",
     "timestamp": 1601115968636,
     "user": {
      "displayName": "Stephen Nesbitt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQLj1729l6nTV5bTwtz3dzPigxe5VgnxpGo5QCiuk=s64",
      "userId": "06234418338196972854"
     },
     "user_tz": 300
    },
    "id": "xrA9BaeEx1Br",
    "outputId": "fb08f700-181b-4867-8ce2-8149c62f9933"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "#you will see we use the same exact method as before with the linear regression\n",
    "reg = RandomForestRegressor().fit(X_train,y_train)\n",
    "\n",
    "#to predict do the following \n",
    "yhat = reg.predict(X_test)\n",
    "\n",
    "#lets plot the results \n",
    "plt.plot(yhat,y_test,'.')\n",
    "\n",
    "mean_abs_error = np.mean(np.abs(y_test - yhat))\n",
    "print('Using RandomForest Reg. gives us a mean error of: {}'.format(mean_abs_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2ZwIAnvyrqP"
   },
   "source": [
    "For this simple 1 feature prediction, the Linear Regression actually does better!\n",
    "\n",
    "### Step 9: \n",
    "\n",
    "Now include temp_2 and the climo in your prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 2525,
     "status": "ok",
     "timestamp": 1601115968638,
     "user": {
      "displayName": "Stephen Nesbitt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQLj1729l6nTV5bTwtz3dzPigxe5VgnxpGo5QCiuk=s64",
      "userId": "06234418338196972854"
     },
     "user_tz": 300
    },
    "id": "qjyyXo1fw4Or",
    "outputId": "eda194a5-1842-4348-a674-6889017d06d3"
   },
   "outputs": [],
   "source": [
    "#so now we need a new X matrix\n",
    "X = np.hstack([df.temp_1[:,np.newaxis],df.temp_2[:,np.newaxis],df.average[:,np.newaxis]])\n",
    "y = df.actual\n",
    "#check shapes\n",
    "X.shape,y.shape\n",
    "#\n",
    "\n",
    "#since we have new feautres, we have to split the data again. \n",
    "#the random state of 42 will ensure we use the same random splits as before\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
    "\n",
    "#see the new sizes\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "executionInfo": {
     "elapsed": 2513,
     "status": "ok",
     "timestamp": 1601115968640,
     "user": {
      "displayName": "Stephen Nesbitt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQLj1729l6nTV5bTwtz3dzPigxe5VgnxpGo5QCiuk=s64",
      "userId": "06234418338196972854"
     },
     "user_tz": 300
    },
    "id": "Uao3bC7xyeB7",
    "outputId": "61eb265d-7378-4fa9-f60e-34e1cefd60f9"
   },
   "outputs": [],
   "source": [
    "#to train this is all we have to do! reg stands for regression\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "#to predict do the following \n",
    "yhat = reg.predict(X_test)\n",
    "\n",
    "#lets plot the results \n",
    "plt.plot(yhat,y_test,'.')\n",
    "\n",
    "mean_abs_error = np.mean(np.abs(y_test - yhat))\n",
    "print('Using Linear Reg. gives us a mean error of: {}'.format(mean_abs_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "executionInfo": {
     "elapsed": 4356,
     "status": "ok",
     "timestamp": 1601115970499,
     "user": {
      "displayName": "Stephen Nesbitt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQLj1729l6nTV5bTwtz3dzPigxe5VgnxpGo5QCiuk=s64",
      "userId": "06234418338196972854"
     },
     "user_tz": 300
    },
    "id": "S3_ZOv-zzwlN",
    "outputId": "db58eccc-b99d-4ed6-ce90-75a723f9c640"
   },
   "outputs": [],
   "source": [
    "#you will see we use the same exact method as before with the linear regression\n",
    "reg = RandomForestRegressor(n_estimators = 1000,).fit(X_train,y_train)\n",
    "\n",
    "#to predict do the following \n",
    "yhat = reg.predict(X_test)\n",
    "\n",
    "#lets plot the results \n",
    "plt.plot(yhat,y_test,'.')\n",
    "\n",
    "mean_abs_error = np.mean(np.abs(y_test - yhat))\n",
    "print('Using RandomForest Reg. gives us a mean error of: {}'.format(mean_abs_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWU7fVSp1HZb"
   },
   "source": [
    "Hmm you can see the RandomForest is still losing. What if we give it the kitchen sink (aka all the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 377,
     "status": "ok",
     "timestamp": 1601117857931,
     "user": {
      "displayName": "Stephen Nesbitt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQLj1729l6nTV5bTwtz3dzPigxe5VgnxpGo5QCiuk=s64",
      "userId": "06234418338196972854"
     },
     "user_tz": 300
    },
    "id": "QD07xq6IzzEl",
    "outputId": "dddedfdf-052a-4d07-f90c-316918e1f85f"
   },
   "outputs": [],
   "source": [
    "df_X =  df.drop(labels=['actual','friend','year_month_day'],axis='columns')\n",
    "\n",
    "#We can also add features, like the day of year, and a proxy for sun angle\n",
    "df_X['doy'] = df['year_month_day'].dt.dayofyear\n",
    "df_X['sun_angle'] = -1.*np.cos(2*np.pi*(21+df_X['doy'])/365.)\n",
    "\n",
    "\n",
    "#so now we need a new X matrix\n",
    "X = df_X.values\n",
    "y = df.actual\n",
    "#check shapes\n",
    "X.shape,y.shape\n",
    "#\n",
    "\n",
    "\n",
    "#since we have new feautres, we have to split the data again. \n",
    "#the random state of 42 will ensure we use the same random splits as before\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
    "\n",
    "\n",
    "#see the new sizes\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "executionInfo": {
     "elapsed": 552,
     "status": "ok",
     "timestamp": 1601117859631,
     "user": {
      "displayName": "Stephen Nesbitt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQLj1729l6nTV5bTwtz3dzPigxe5VgnxpGo5QCiuk=s64",
      "userId": "06234418338196972854"
     },
     "user_tz": 300
    },
    "id": "tfZMOkII1bgb",
    "outputId": "29ba1037-5a43-450c-ffc8-c2ed0b93e4f2"
   },
   "outputs": [],
   "source": [
    "#to train this is all we have to do! reg stands for regression\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "#to predict do the following \n",
    "yhat = reg.predict(X_test)\n",
    "\n",
    "#lets plot the results \n",
    "plt.plot(yhat,y_test,'.')\n",
    "\n",
    "mean_abs_error = np.mean(np.abs(y_test - yhat))\n",
    "print('Using Linear Reg. gives us a mean error of: {}'.format(mean_abs_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "executionInfo": {
     "elapsed": 2616,
     "status": "ok",
     "timestamp": 1601117863661,
     "user": {
      "displayName": "Stephen Nesbitt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQLj1729l6nTV5bTwtz3dzPigxe5VgnxpGo5QCiuk=s64",
      "userId": "06234418338196972854"
     },
     "user_tz": 300
    },
    "id": "xtA4i8-O1fSL",
    "outputId": "f0b1fcab-e21d-4f24-c851-b239b8376fae"
   },
   "outputs": [],
   "source": [
    "#you will see we use the same exact method as before with the linear regression\n",
    "reg = RandomForestRegressor(n_estimators = 1000, random_state = 42).fit(X_train,y_train)\n",
    "\n",
    "#to predict do the following \n",
    "yhat = reg.predict(X_test)\n",
    "\n",
    "#lets plot the results \n",
    "plt.plot(yhat,y_test,'.')\n",
    "\n",
    "mean_abs_error = np.mean(np.abs(y_test - yhat))\n",
    "print('Using RandomForest Reg. gives us a mean error of: {}'.format(mean_abs_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lE2jEUU384hB"
   },
   "source": [
    "There are also utilities to do a parameter sweep to do see the effect of hyperparameters on your model.  We won't do this here because it takes a lot of time, but here is code in case you are interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 253,
     "status": "ok",
     "timestamp": 1601077157808,
     "user": {
      "displayName": "Stephen Nesbitt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQLj1729l6nTV5bTwtz3dzPigxe5VgnxpGo5QCiuk=s64",
      "userId": "06234418338196972854"
     },
     "user_tz": 300
    },
    "id": "uHzn5Gs62lW7"
   },
   "outputs": [],
   "source": [
    "n_estimators = [500, 800, 1500 1000, 2000]\n",
    "\n",
    "max_features = ['auto','sqrt','log2']\n",
    "\n",
    "max_depth = [10, 25, 50]\n",
    "max_depth.append(None)\n",
    "\n",
    "min_samples_split = [2, 10, 20]\n",
    "\n",
    "min_samples_leaf = [1, 4, 15]\n",
    "\n",
    "grid_param = {'n_estimators':n_estimators,\n",
    "              'max_features':max_features,\n",
    "              'max_depth':max_depth,\n",
    "              'min_samples_split':min_samples_split,\n",
    "              'min_samples_leaf':min_samples_leaf}\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "reg = RandomForestRegressor(random_state = 42)\n",
    "reg_random = RandomizedSearchCV(estimator = reg,\n",
    "                                param_distributions = grid_param, n_iter = 500,\n",
    "                                cv = 5, verbose = 2, random_state = 42,\n",
    "                                n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-j_dKTb9LnS"
   },
   "source": [
    "Now, do the parameter sweep on all available CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "executionInfo": {
     "elapsed": 2186563,
     "status": "ok",
     "timestamp": 1601079348213,
     "user": {
      "displayName": "Stephen Nesbitt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQLj1729l6nTV5bTwtz3dzPigxe5VgnxpGo5QCiuk=s64",
      "userId": "06234418338196972854"
     },
     "user_tz": 300
    },
    "id": "neFsDrd0a-9j",
    "outputId": "735ab92b-46a9-40e1-e28e-55bc78b59776"
   },
   "outputs": [],
   "source": [
    "reg_random.fit(X_train,y_train)\n",
    "print(reg_random.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0xkU9IX-Wxt"
   },
   "source": [
    "Here is output, you can see it took some time.  It returns the optimal parameters that it found.\n",
    "\n",
    "```\n",
    "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
    "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 324 is smaller than n_iter=500. Running 324 iterations. For exhaustive searches, use GridSearchCV.\n",
    "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
    "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
    "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   55.7s\n",
    "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:  3.8min\n",
    "[Parallel(n_jobs=-1)]: Done 361 tasks      | elapsed:  8.1min\n",
    "[Parallel(n_jobs=-1)]: Done 644 tasks      | elapsed: 14.6min\n",
    "[Parallel(n_jobs=-1)]: Done 1009 tasks      | elapsed: 22.8min\n",
    "[Parallel(n_jobs=-1)]: Done 1454 tasks      | elapsed: 32.9min\n",
    "[Parallel(n_jobs=-1)]: Done 1620 out of 1620 | elapsed: 36.4min finished\n",
    "{'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 10}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yD1kKSEZF0QD"
   },
   "source": [
    "So how did we do compared with the \"experts\"?  Let's compare the models with the forecasts we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "executionInfo": {
     "elapsed": 402,
     "status": "ok",
     "timestamp": 1601118338392,
     "user": {
      "displayName": "Stephen Nesbitt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQLj1729l6nTV5bTwtz3dzPigxe5VgnxpGo5QCiuk=s64",
      "userId": "06234418338196972854"
     },
     "user_tz": 300
    },
    "id": "K1JoUPVlAvL7",
    "outputId": "d26d2627-4085-40ee-dcf9-7934bdd5fbf3"
   },
   "outputs": [],
   "source": [
    "print('NOAA MAE: ',np.mean(np.abs(df['forecast_noaa']-df['actual'])))\n",
    "print('Under MAE: ',np.mean(np.abs(df['forecast_under']-df['actual'])))\n",
    "print('AccU MAE: ',np.mean(np.abs(df['forecast_acc']-df['actual'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6RBCjsvCtRf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "ML_Regression_Dem.ipynb",
   "provenance": [
    {
     "file_id": "1kuE4x5N06sIlf2l6XoIx_NU0b3STSpQa",
     "timestamp": 1601073009790
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
